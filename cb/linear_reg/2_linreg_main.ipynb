{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from copy import copy\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.ml.evaluation import Evaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.functions import concat, col, lit, rand\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    r2_score,\n",
    "    precision_recall_curve\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6af8a8",
   "metadata": {},
   "source": [
    "## Global Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b34c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "KycDM: str = 'zt_dm_kyc_data'            # Схема KYC на чтение\n",
    "arfsDM: str = 'zt_dm_aso_dfm_arfs'       # Схема ARFS, с правами на запись. Таблица создается сюда\n",
    "target_table: str = 'tmp_target_table'   # таблица полученная из крансых и зеленых клиентов\n",
    "    \n",
    "filter_date: str = '2021-12-04'          # фильтр для fct_**\n",
    "max_score: int = 1000                    # Максимальное значение риска\n",
    "middle: int = 500                        # \"Среднее\" рисковое значение\n",
    "uncertanity: int = 150                   # Значение окрестности для желтых, 0 - нет желтых\n",
    "\n",
    "PATH: str = r'../FR/PythinProjects/ML_weights/'\n",
    "\n",
    "ignored_typolygy: List[int] = [6, ]      # пропущенные типологии\n",
    "ignored_columns: List[str] = [\n",
    "    'fct_shady_risks',\n",
    "]\n",
    "ignored_scenatios: List[str] = [\n",
    "    'R01.51 Белый список',\n",
    "    'R01.52 Исполнители государственных контрактов',\n",
    "    'R01.53 Участники внешнеэкономической деятельности',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get calendar id\n",
    "try:\n",
    "    calendar_id = spark.sql(f'select id from {KycDM}.dim_calendar where day = {filter_date}').collection()\n",
    "except:\n",
    "    calendar_id = None\n",
    "print(f'Choose calendar_id: {calendar_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4fb981",
   "metadata": {},
   "source": [
    "# Visualition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, probs):\n",
    "    fpr, tpr, _ = roc_curve(y_true, probs)\n",
    "    auc = roc_auc_score(y_true, probs)\n",
    "    \n",
    "    plt.figure(figsize=[7, 6])\n",
    "    plt.plot(fpr, tpr, color='darkorange', label=f'ROC curve (area = {auc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('ROC-curve')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "def plot_pr_curve(y_true, probs):\n",
    "    fpr, tpr, _ = precision_recall_curve(y_true, probs)\n",
    "    \n",
    "    plt.plot(fpr, tpr, color='darkorange')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.05])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('ROC-curve')\n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa1c079",
   "metadata": {},
   "source": [
    "# portfolio recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6914c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_dataframe(target_df, typology, verbose=False):\n",
    "    table_names = list(set([risk['table_name'] for risk in typology['risks'] if not risk['table_name'] == 'fct_shady_risks']))\n",
    "    total_rist: int = 0\n",
    "    \n",
    "    for table_name in table_names:\n",
    "        risk_columns = [risk['column_name'] for risk in typology['risks'] if risk['table_name'] == table_name]\n",
    "        total_risks += len(risk_columns)\n",
    "        if verbose:\n",
    "            print(f'{table_name}:\\t\\t\\t{len(risk_columns)} columns')\n",
    "        \n",
    "        df = spark.sql(f'select * from {KycDM}.{table_name}')\n",
    "        \n",
    "        # Проверяем наличие всех необходимых сценариев в fct_ таблицах\n",
    "        absent_columns = [r for r in risk_columns if not r in df.schema.names]\n",
    "        \n",
    "        if len(absent_columns) > 0:\n",
    "            raise Exception(f'Cant find {\";\".join(absent_columns)} in {KycDM}.{table_name}')\n",
    "        \n",
    "        # Если объявлен calander_id, фильтруем таблицы\n",
    "        if calendar_id and 'calendar_id' in df.scema.names:\n",
    "            df = df.filter(f'calendar_id = {calendar_id}')\n",
    "        \n",
    "        # Проверяем пустая ли таблица\n",
    "        if df.count() == 0:\n",
    "            print(f'Warning\\t\\t{table_name} is empty')\n",
    "        \n",
    "        select_columns = [f'{column_name} as {table_name}__{column_name}' for column_name in risk_columns]\n",
    "        \n",
    "        select_columns = ['client_id', ] + select_columns\n",
    "        \n",
    "        df = df.selectExpr(*select_columns)\n",
    "        \n",
    "        if not target_df:\n",
    "            target_df = df\n",
    "            continue\n",
    "        \n",
    "        target_df = target_df.join(df, on=['client_id',], how='inner')\n",
    "    \n",
    "    if verbose:\n",
    "        print('0'*50, f'\\nTotal Columns: {total_risks}')\n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd341d50",
   "metadata": {},
   "source": [
    "## Create DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a89280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(typology, return_assembler=False):\n",
    "    print(f\"{typology['dubious_id']} - {typology['description']}\\n\")\n",
    "    \n",
    "    # get only \"красные\"\n",
    "    target_df = spark.sql(f'select client_id, label, from {arfsDM}.{target_table} where dubious_id = {typology[\"dubious_id\"]}')\n",
    "    \n",
    "    # get onlu 'зеленые'\n",
    "    greens = spark.sql(f'select client_id, table from {arfsDM}.{target_table} where label = 0')\n",
    "    \n",
    "    # balanced and join\n",
    "    green_fraction = target_df.count() / greens.count() * 1.2\n",
    "    target_df = target_df.union(greens.sample(green_fraction).limit(target_df.count()))\n",
    "    \n",
    "    # !\n",
    "    df = feature_dataframe(target_df, typology, verbose=True)\n",
    "    df = df.fillna(False)\n",
    "    df = df.na.fill('false') # на сулчай пустых таблиц\n",
    "    \n",
    "    print('\\t\\t\\tБаланс классов')\n",
    "    df.groupby('label').count().show()\n",
    "    \n",
    "    # Transformation to Boolean\n",
    "    print('Преобразование типов')\n",
    "    for column in df.columns[2:]:\n",
    "        df = df.withColumn(column, col(column).cast('Boolearn'))\n",
    "        \n",
    "    print('Сборка колончатых признаков')\n",
    "    inputCols = copy(df.schema.names)\n",
    "    inputCols.remove('label')\n",
    "    inputCols.remove('client_id')\n",
    "    \n",
    "    # Объеденим все столбцы кроме 'label' & 'client_id'\n",
    "    assembler = VectorAssembler().setInputCols(inputCols).setOutputCol('features')\n",
    "    assembler_df = assembler.transform(df)\n",
    "    \n",
    "    try:\n",
    "        print(f'Сохранение в {arfsDM}.tmp_train_test_dataset')\n",
    "        assembler_df.write.mode('overwrite').saveAsTable(f'{arfsDM}.tmp_train_test_dataset')\n",
    "        \n",
    "        print('DONE')\n",
    "        assembler_df = spark.sql(f'select * from {arfsDM}.tmp_train_test_dataset')\n",
    "        \n",
    "        if return_assembler:\n",
    "            return assembler_df, assembler\n",
    "        else:\n",
    "            assembler_df\n",
    "    except Exception as e:\n",
    "        print(f\"{typology['dubious_id']}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdda4683",
   "metadata": {},
   "source": [
    "# Custom class for Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275922cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEvaluator(Evaluator):\n",
    "    # тк лэйбл кодируется 0 или 1000, делаем костомный фильтр\n",
    "    label_true: int = 1000\n",
    "    label_false: int = 0\n",
    "    predict_middle: int = 500 # для придикта далем пороговое значение для фильтра > 650 - красный клиент\n",
    "        \n",
    "    def __init__(self, predictionCol=None, labelCol=None, metric=None) -> None:\n",
    "        self.predictionCol = predictionCol\n",
    "        self.labelCol = labelCol\n",
    "        self.metric = metric\n",
    "    \n",
    "    def _evaluate(self, dataset) -> float:\n",
    "        if self.metric == 'f1':\n",
    "            return self.__f1(dataset)\n",
    "        elif self.metric == 'accuracy':\n",
    "            return self.__acuracy(dataset)\n",
    "        elif self.metric == 'precision':\n",
    "            return self.__precision(dataset)\n",
    "        elif self.metric == 'recal':\n",
    "            return self.__recal(dataset)\n",
    "        else:\n",
    "            print('Metric not aavalible\\nAccuracy by default')\n",
    "            return self.__acuracy(dataset)\n",
    "    \n",
    "    def isLargerBetter(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def __f1(self, dataset) -> float:\n",
    "        tp = dataset.filter((F.col(self.labelCol) == self.label_true) & (F.col(self.predictionCol) > self.predict_middle)).count()\n",
    "        fp = dataset.filter((F.col(self.labelCol) == self.label_false) & (F.col(self.predictionCol) < self.predict_middle)).count()\n",
    "        tn = dataset.filter((F.col(self.labelCol) == self.label_true) & (F.col(self.predictionCol) < self.predict_middle)).count()\n",
    "        return (2 * tp) / (2 * tp + fp + tn)\n",
    "    \n",
    "    def __accuracy(self, dataset) -> float:\n",
    "        tp = dataset.filter((F.col(self.labelCol) == self.label_true) & (F.col(self.predictionCol) > self.predict_middle)).count()\n",
    "        tn = dataset.filter((F.col(self.labelCol) == self.label_true) & (F.col(self.predictionCol) < self.predict_middle)).count()\n",
    "        return (tp + tn) / dataset.count()\n",
    "    \n",
    "    def __precision(self, dataset) -> float:\n",
    "        tp = dataset.filter((F.col(self.labelCol) == self.label_true) & (F.col(self.predictionCol) > self.predict_middle)).count()\n",
    "        fp = dataset.filter((F.col(self.labelCol) == self.label_true) & (F.col(self.predictionCol) > self.predict_middle)).count()\n",
    "        return tp / (tp + fp)\n",
    "    \n",
    "    def __recal(self, dataset) -> float:\n",
    "        tp = dataset.filter((F.col(self.labelCol) == self.label_true) & (F.col(self.predictionCol) > self.predict_middle)).count()\n",
    "        fn = dataset.filter((F.col(self.labelCol) == self.label_false) & (F.col(self.predictionCol) < self.predict_middle)).count()\n",
    "        return tp / (tp + fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036f26c",
   "metadata": {},
   "source": [
    "# Learn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(train, num_folds: int = 5):\n",
    "    '''\n",
    "        input:\n",
    "            - train - train dataset\n",
    "            - num_folds - count folds\n",
    "        output:\n",
    "            - model with best parametrs\n",
    "    '''\n",
    "    \n",
    "    pool: int = 6 # кол-во потоков\n",
    "        \n",
    "    estimator = LinearRegression(featuresCol='features', labelCol='label', fitIntercept=False)\n",
    "    \n",
    "    estimator_grid = ParamGridBuilder().\\\n",
    "        addGrid(estimator.regParam, np.linspace(0.01, 0.001, 10)).\\\n",
    "        addGrid(estimator.elasticnetParam, np.linspace(0, 1, 10)).\\\n",
    "        build()\n",
    "    \n",
    "    # Костомные метрики для регрессионной модели\n",
    "    evalutor = CustomEvaluator(predictionCol='prediction', labelCol='label', metric='f1')\n",
    "    \n",
    "    print(f'Start Validation with: {num_folds} folds')\n",
    "    start = time.time()\n",
    "    \n",
    "    # кроссвалидация с поиском по сетке\n",
    "    cvModel = CrossValidator(\n",
    "        estimator=estimator,\n",
    "        estimatorParamMaps=estimator_grid,\n",
    "        evaluator=evalutor,\n",
    "        numFolds=num_folds,\n",
    "        parallelism=pool,\n",
    "    )\n",
    "    model = cvModel.fit(train)\n",
    "\n",
    "    print(f'\\nTime crossval: {time.time() - start}')\n",
    "\n",
    "    # освобождаем память\n",
    "    del cvModel\n",
    "    \n",
    "    print(f'\\nF1:\\t{model.avgMetrics}')\n",
    "    \n",
    "    # get best params\n",
    "    hyper_dict = dict()\n",
    "    hyper_params = model.getEstimatorParamMaps()[np.argmax(model.avgMetrics)]\n",
    "    \n",
    "    for i in range(len(hyper_params.items())):\n",
    "        hyper_name = re.search(\"name='(.+?)'\", str([x for x in hyper_params.items()][i])).group(1)\n",
    "        hyper_value = [x for x in hyper_params.items()][i][1]\n",
    "        hyper_dict[hyper_name] = hyper_value\n",
    "    \n",
    "    print(f'\\n\\nBest model: {hyper_dict}\\nMetric: {max(model.avgMetrics):.3f}')\n",
    "    \n",
    "    spark.catalog.clearCache()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee325be",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48628cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test, description:str=''):\n",
    "    best_model = model.bestModel\n",
    "    \n",
    "    test_df = best_model.transform(test).select('label', 'prediction').toPandas()\n",
    "    \n",
    "    # features enginering\n",
    "    test_dt['label'] = test_df['label'].apply(lambda x: 1 if x == 1000 else 0)\n",
    "    test_dt['prediction_true'] = test_df['prediction'].apply(lambda x: 1000 if x > 1000 else x)\n",
    "    test_dt['prediction_true'] = test_df['prediction_true'].apply(lambda x: 0 if x < 0 else x)\n",
    "    test_dt['probability'] = test_df['prediction_true'] / max_score\n",
    "    test_dt['prediction_true'] = test_df['prediction_true'].apply(lambda x: 1 if x >= 650 else 0)\n",
    "    \n",
    "    y_true = test_df['label']\n",
    "    y_pred = test_df['prediction_true']\n",
    "    y_proba = test_df['probability']\n",
    "    \n",
    "    # regression mertics\n",
    "    rmse = model.bestModel.summary.rootMeanSquaredError\n",
    "    r2 = model.bestModel.summary.r2\n",
    "    \n",
    "    # classsification metrics\n",
    "    precision = precision(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    auc_roc = roc_auc_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    gini = 2 * auc_roc - 1\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=[1, 0])\n",
    "    \n",
    "    print('\\n====Metrics on test dataset=====')\n",
    "    print(f'\\tF1:\\t\\t{f1:.3f}')\n",
    "    print(f'\\tAccuracy:\\t{accuracy:.3f}')\n",
    "    print(f'Precision:\\t{precision:.3f}')\n",
    "    print(f'AUC-ROC:\\t{auc_roc:.3f}')\n",
    "    print(f'Gini:\\t\\t{gini:.3f}')\n",
    "    print(f'RMSE:\\t\\t{rmse:.3f}')\n",
    "    print(f'r2:\\t\\t{r2:.3f}')\n",
    "    print('*' * 40)\n",
    "    print(f'Confusion matrix:\\n{conf_matrix}')\n",
    "    print('*' * 40)\n",
    "    \n",
    "    plot_roc_curve(y_true, y_proba, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0491a79a",
   "metadata": {},
   "source": [
    "## Get Typolygies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da92161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_columns_query = f\"\\\n",
    "    SELECT DISTINCT w.ID, w.DUBIOUS_ID, d.description as dubiuos_description, w.table_name, w.column_name, r.scenario_code, r.description as scenario_description \\\n",
    "    FROM {KycDM}.hw_dubious_weight as w \\\n",
    "    LEFT JOIN {KycDM}.dim_risk_columns as r \\\n",
    "    ON w.table_name = r.table_name and w.column_name = r.column_name \\\n",
    "    LEFT JOIN {KycDM}.dim_dubious as d \\\n",
    "    ON w.dubious_id = d.id \\\n",
    "    \"\n",
    "risks_columns = spark.sql(risk_columns_query)\n",
    "\n",
    "#global list of Typologies\n",
    "weights = []\n",
    "dubious = risks_columns.select(col('dubious_id'), col('dubiuos_description')).distinct().collect()\n",
    "\n",
    "for dub_id, description in dubious:\n",
    "    typology = {\n",
    "        \"dubious_id\": dub_id,\n",
    "        \"description\": description,\n",
    "        'medium_ppm': int(middle - uncertanity),\n",
    "        'high_ppm': int(middle + uncertanity),\n",
    "        'risks': [],\n",
    "    }\n",
    "    weights.append(typology)\n",
    "\n",
    "# adding list of risks to each typology\n",
    "for typology in weights:\n",
    "    dubious_id = typology['dubious_id']\n",
    "    for (risk_id, table_name, column_name, description) in risks_columns.filter(\n",
    "        f\"dubious_id = {dubious_id}\").select(\"ID\", 'table_name', 'column_name', 'scenario_description').collect() :\n",
    "        \n",
    "        # Выставляем максимальный риск для fct_shady_risks, т.к. это и есть спец. перечни. НЕ будет участвовать в обучении.\n",
    "        weight = max_score if table_name == \"fct_shady_risks\" else 0\n",
    "        typology['risks'].append(\n",
    "            {\n",
    "                'id': risk_id,\n",
    "                'table_name': table_name,\n",
    "                'column_name': column_name,\n",
    "                'description': description, \n",
    "                'weight'     : weight,\n",
    "            })\n",
    "            \n",
    "print(f\"Всего типологий риска: {len(weights)}\\n\")\n",
    "for t in weights:\n",
    "    print(f\"id{t['dubious_id']} - {t['description'][:30]}: {len(t['risks'])} сценариев\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa3c7d",
   "metadata": {},
   "source": [
    "## Learn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26618a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for typology in weights:\n",
    "    #пропускаем неиспользуемые типологии\n",
    "    if typology['dubious_id'] in ignored_typology:\n",
    "        continue\n",
    "\n",
    "    dataset, assembler = create_dataset(typology, return_assembler=True)\n",
    "\n",
    "    train, test = dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "        \n",
    "    print(\"\\nОбучение моделей...\")\n",
    "    model = create_model(train, num_folds=5)\n",
    "    \n",
    "    columns = test.columns[2:-1]\n",
    "    \n",
    "    print('=' * 28, 'Тестирование модели', '=' * 28)\n",
    "    title_plot = f'{typology[\"dubious_id\"]} - {typology[\"description\"]}'\n",
    "    test_model(model, test, title_plot)\n",
    "    \n",
    "    print(f'\\n\\tHyperParameters Statistic\\n{get_stats(model)}')\n",
    "    \n",
    "    print('\\nValue wieghts for each scenarios')\n",
    "    weights_df = get_weights(model, columns)\n",
    "    print(weights_df.head())\n",
    "    \n",
    "    print('\\n\\t Add weights to dict\\n')\n",
    "    for risk in typology['risks']:\n",
    "        for ind, row in weights_df.iterrows():\n",
    "            if risk['column_name'] in ignored_columns:\n",
    "                continue\n",
    "            if risk['table_name'] + '__' + risk['column_name'] == row['column']:\n",
    "                rosk['weight'] = round(row['coef'], 2)\n",
    "    del weights_df\n",
    "    \n",
    "    #clear cache\n",
    "    spark.catalog.clearCache()\n",
    "print('=' * 30, 'Done', '*' * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff95d4ec",
   "metadata": {},
   "source": [
    "# delete 'client_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5787d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight in weights:\n",
    "    weight.pop('client_type', None)\n",
    "    for risk in weight['risks']:\n",
    "        risk.pop('client_type', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f39ec0e",
   "metadata": {},
   "source": [
    "## Load weights to excel-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9147fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "\n",
    "for row in weights:\n",
    "    for risk in row['risks']:\n",
    "        dct = {}\n",
    "        dct['dubious_id'] = row['dubious_id']\n",
    "        dct['typology'] = row['typology']\n",
    "        dct['description'] = risk['description']\n",
    "        dct['weight'] = risk['weight']\n",
    "        lst.append(dct)\n",
    "\n",
    "pd.DataFrame(lst).to_excel(f'{PATH}/typologies_{filter_date}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01a6bf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
