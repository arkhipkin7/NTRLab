{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from copy import copy\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.sql.functions import col, lit, rand\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfcccc1",
   "metadata": {},
   "source": [
    "## Check Spark connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "conf = SparkConf().setAppName('spark-shell')\n",
    "conf.set('spark.driver.memory', '4g')\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6af8a8",
   "metadata": {},
   "source": [
    "## Global Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b34c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "KycDM: str = 'zt_dm_kyc_data'            # Схема KYC на чтение\n",
    "arfsDM: str = 'zt_dm_aso_dfm_arfs'       # Схема ARFS, с правами на запись. Таблица создается сюда\n",
    "target_table: str = 'tmp_target_table'   # таблица полученная из крансых и зеленых клиентов\n",
    "    \n",
    "filter_date: str = '2021-07-22'          # фильтр для fct_**\n",
    "max_score: int = 1000                    # Максимальное значение риска\n",
    "middle: int = 500                        # \"Среднее\" рисковое значение\n",
    "uncertanity: int = 150                   # Значение окрестности для желтых, 0 - нет желтых\n",
    "ignored_typolygy: List[int] = [6, ]      # пропущенные типологии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5a83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get calendar id\n",
    "\n",
    "try:\n",
    "    calendar_id = spark.sql(f'select id from {KycDM}.dim_calendar where day = {filter_date}').collection()\n",
    "except:\n",
    "    calendar_id = None\n",
    "print(f'Choose calendar_id: {calendar_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4fb981",
   "metadata": {},
   "source": [
    "# Visualition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c65a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "\n",
    "from plotly.graph_objs import Scatter, Layout\n",
    "\n",
    "\n",
    "def plot(plot_dct, height=500, width=500, **kwargs):\n",
    "    kwargs['output_type'] = 'div'\n",
    "    plot_str = plotly.offline.plot(plot_dct, **kwargs)\n",
    "    print(f'Angular <div style=\"height: {height}px; wigth: {width}px\"> {plot_str} </div>')\n",
    "\n",
    "    \n",
    "def plotxy(x, y):\n",
    "    plot({\n",
    "        'data': [\n",
    "            Scatter(x=x, y=y)\n",
    "        ],\n",
    "        'layout': {'title': {'text': ''}}\n",
    "    })\n",
    "    \n",
    "    \n",
    "def plot_roc_curve(y_true, y_prob, text=''):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plot({\n",
    "        'data': [\n",
    "            Scatter(x=fpr, y=tpr)\n",
    "        ],\n",
    "        'layout': {'title': {'text': 'ROC Кривая ' + text}}\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa1c079",
   "metadata": {},
   "source": [
    "# portfolio recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6914c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_dataframe(target_df, typology, verbose=False):\n",
    "    table_names = list(set([risk['table_name'] for risk in typology['risks'] if not risk['table_name'] == 'fct_shady_risks']))\n",
    "    total_rist: int = 0\n",
    "    \n",
    "    for table_name in table_names:\n",
    "        risk_columns = [risk['column_name'] for risk in typology['risks'] if risk['table_name'] == table_name]\n",
    "        total_risks += len(risk_columns)\n",
    "        if verbose:\n",
    "            print(f'{table_name}:\\t\\t\\t{len(risk_columns)} columns')\n",
    "        \n",
    "        df = spark.sql(f'select * from {KycDM}.{table_name}')\n",
    "        \n",
    "        # Проверяем наличие всех необходимых сценариев в fct_ таблицах\n",
    "        absent_columns = [r for r in risk_columns if not r in df.schema.names]\n",
    "        \n",
    "        if len(absent_columns) > 0:\n",
    "            raise Exception(f'Cant find {\";\".join(absent_columns)} in {KycDM}.{table_name}')\n",
    "        \n",
    "        # Если объявлен calander_id, фильтруем таблицы\n",
    "        if calendar_id and 'calendar_id' in df.scema.names:\n",
    "            df = df.filter(f'calendar_id = {calendar_id}')\n",
    "        \n",
    "        # Проверяем пустая ли таблица\n",
    "        if df.count() == 0:\n",
    "            print(f'Warning\\t\\t{table_name} is empty')\n",
    "        \n",
    "        select_columns = [f'{column_name} as {table_name}__{column_name}' for column_name in risk_columns]\n",
    "        \n",
    "        select_columns = ['client_id', ] + select_columns\n",
    "        \n",
    "        df = df.selectExpr(*select_columns)\n",
    "        \n",
    "        if not target_df:\n",
    "            target_df = df\n",
    "            continue\n",
    "        \n",
    "        target_df = target_df.join(df, on=['client_id',], how='inner')\n",
    "    \n",
    "    if verbose:\n",
    "        print('0'*50, f'\\nTotal Columns: {total_risks}')\n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd341d50",
   "metadata": {},
   "source": [
    "## Create DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a89280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(typology, return_assembler=False):\n",
    "    print(f\"{typology['dubious_id']} - {typology['description']}\\n\")\n",
    "    \n",
    "    # get only \"красные\"\n",
    "    target_df = spark.sql(f'select client_id, label, from {arfsDM}.{target_table} where dubious_id = {typology[\"dubious_id\"]}')\n",
    "    \n",
    "    # get onlu 'зеленые'\n",
    "    greens = spark.sql(f'select client_id, table from {arfsDM}.{target_table} where label = 0')\n",
    "    \n",
    "    # balanced and join\n",
    "    green_fraction = target_df.count() / greens.count() * 1.2\n",
    "    target_df = target_df.union(greens.sample(green_fraction).limit(target_df.count()))\n",
    "    \n",
    "    # !\n",
    "    df = feature_dataframe(target_df, typology, verbose=True)\n",
    "    df = df.fillna(False)\n",
    "    df = df.na.fill('false') # на сулчай пустых таблиц\n",
    "    \n",
    "    print('\\t\\t\\tБаланс классов')\n",
    "    df.groupby('label').count().show()\n",
    "    \n",
    "    # Transformation to Boolean\n",
    "    print('Преобразование типов')\n",
    "    for column in df.columns[2:]:\n",
    "        df = df.withColumn(column, col(column).cast('Boolearn'))\n",
    "        \n",
    "    print('Сборка колончатых признаков')\n",
    "    inputCols = copy(df.schema.names)\n",
    "    inputCols.remove('label')\n",
    "    inputCols.remove('client_id')\n",
    "    \n",
    "    # Объеденим все столбцы кроме 'label' & 'client_id'\n",
    "    assembler = VectorAssembler().setInputCols(inputCols).setOutputCol('features')\n",
    "    assembler_df = assembler.transform(df)\n",
    "    \n",
    "    try:\n",
    "        print(f'Сохранение в {arfsDM}.tmp_train_test_dataset')\n",
    "        assembler_df.write.mode('overwrite').saveAsTable(f'{arfsDM}.tmp_train_test_dataset')\n",
    "        \n",
    "        print('DONE')\n",
    "        assembler_df = spark.sql(f'select * from {arfsDM}.tmp_train_test_dataset')\n",
    "        \n",
    "        if return_assembler:\n",
    "            return assembler_df, assembler\n",
    "        else:\n",
    "            assembler_df\n",
    "    except Exception as e:\n",
    "        print(f\"{typology['dubious_id']}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036f26c",
   "metadata": {},
   "source": [
    "# Learn Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc064a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train, test):\n",
    "    models = []\n",
    "    \n",
    "    lr = LinearRegression(featuresCol='features', labelCol='label', fitIntercept=False)\n",
    "    \n",
    "    lr_grid = ParamGridBuilder().addGrid(lr.regParam, [0., 0.02]).addGrid(lr.elasticnetParam, [0., 0.4]).build()\n",
    "    \n",
    "    for i in range(len(lr_grid)):\n",
    "        params = lr_grid[i]\n",
    "        models.append({'id': f'LR_{i}', \n",
    "                          'parent': lr,\n",
    "                          'type': 'lr',\n",
    "                          'params': params,\n",
    "                          'description': str({k.name: v for k, v in params.items()})\n",
    "                          })\n",
    "    for model in models:\n",
    "        print(f\"Fitting {model['id']}: {model['description']}\", end=' ')\n",
    "        params = model['params']\n",
    "        start = datetime.now()\n",
    "        model['model'] = model['parent'].fit(train, params)\n",
    "        delta = (datetime.now() - start).seconds\n",
    "        print(f'{delta // 60} min {delta % 60} sec')\n",
    "    print('Fitting Done\\nОценка модели...')\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        print('-'*20, f\"\\nEvaluating Model {model['id']}: {model['description']}\")\n",
    "        predictions = model['model'].transform(test)\n",
    "        preds = predictions.select('client_id', 'label', 'prediction').toPandas()\n",
    "        preds['label_bin'] = preds.label > 0\n",
    "        preds['max_score'] = max_score\n",
    "        preds['prediction'] = preds[['prediction', 'max_score']].min(axis=1)\n",
    "        preds['probability'] = preds.prediction / max_score\n",
    "        \n",
    "        y_true = preds.label_bin\n",
    "        \n",
    "        thresholds = range(max_score)\n",
    "        f1s = []\n",
    "        y_true = preds.label > 0\n",
    "        for th in thresholds:\n",
    "            y_pred = preds.predictions > th\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            f1s.append(f1)\n",
    "        \n",
    "        threshold = f1s.index(max_score)\n",
    "        best_f1 = max(f1s)\n",
    "        \n",
    "        preds['prediction_bin'] = preds.predictions > threshold\n",
    "        \n",
    "        y_pred = preds.prediction_bin\n",
    "        \n",
    "        cnf_matrix = confusion_matrix(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0491a79a",
   "metadata": {},
   "source": [
    "## Get Typolygies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da92161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_columns_query = f\"\\\n",
    "    SELECT DISTINCT w.ID, w.DUBIOUS_ID, d.description as dubiuos_description, w.table_name, w.column_name, r.scenario_code, r.description as scenario_description \\\n",
    "    FROM {KycDM}.hw_dubious_weight as w \\\n",
    "    LEFT JOIN {KycDM}.dim_risk_columns as r \\\n",
    "    ON w.table_name = r.table_name and w.column_name = r.column_name \\\n",
    "    LEFT JOIN {KycDM}.dim_dubious as d \\\n",
    "    ON w.dubious_id = d.id \\\n",
    "    \"\n",
    "risks_columns = spark.sql(risk_columns_query)\n",
    "\n",
    "#global list of Typologies\n",
    "weights = []\n",
    "dubious = risks_columns.select(col('dubious_id'), col('dubiuos_description')).distinct().collect()\n",
    "\n",
    "for dub_id, description in dubious:\n",
    "    typology = {\n",
    "        \"dubious_id\": dub_id,\n",
    "        \"description\": description,\n",
    "        'medium_ppm': int(middle - uncertanity),\n",
    "        'high_ppm': int(middle + uncertanity),\n",
    "        'risks': [],\n",
    "    }\n",
    "    weights.append(typology)\n",
    "\n",
    "# adding list of risks to each typology\n",
    "for typology in weights:\n",
    "    dubious_id = typology['dubious_id']\n",
    "    for (risk_id, table_name, column_name, description) in risks_columns.filter(\n",
    "        f\"dubious_id = {dubious_id}\").select(\"ID\", 'table_name', 'column_name', 'scenario_description').collect() :\n",
    "        \n",
    "        # Выставляем максимальный риск для fct_shady_risks, т.к. это и есть спец. перечни. НЕ будет участвовать в обучении.\n",
    "        weight = max_score if table_name == \"fct_shady_risks\" else 0\n",
    "        typology['risks'].append(\n",
    "            {\n",
    "                'id': risk_id,\n",
    "                'table_name': table_name,\n",
    "                'column_name': column_name,\n",
    "                'description': description, \n",
    "                'weight'     : weight,\n",
    "            })\n",
    "            \n",
    "print(f\"Всего типологий риска: {len(weights)}\\n\")\n",
    "for t in weights:\n",
    "    print(f\"id{t['dubious_id']} - {t['description'][:30]}: {len(t['risks'])} сценариев\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa3c7d",
   "metadata": {},
   "source": [
    "## Learn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26618a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for typology in weights:\n",
    "    #пропускаем неиспользуемые типологии\n",
    "    if typology['dubious_id'] in ignored_typology:\n",
    "        continue\n",
    "\n",
    "    dataset, assembler = create_dataset(typology, return_assembler=True)\n",
    "    train, test = dataset.randomSplit([0.8, 0.2], seed = 100500)\n",
    "        \n",
    "    print(\"\\nОбучение моделей...\")\n",
    "    model = train_model(train, test)\n",
    "    \n",
    "    #Сохранение полученной модели в словарь\n",
    "    m = {\n",
    "        \"dubious_id\": typology['dubious_id'],\n",
    "        \"model\": model,\n",
    "        \"assembler\": assembler,\n",
    "    }\n",
    "        \n",
    "    models[typology['dubious_id']] = m\n",
    "    \n",
    "    print(\"\\nЛучшая модель:\")\n",
    "    print(f\"f1 score: {model['f1']}, \\nAUC_ROC score: {round(model['auc_roc'], 3)}, \\ngini: {model['gini']}\")\n",
    "\n",
    "    # Извлечение весов\n",
    "    lrModel = model['model']\n",
    "    coeffs = lrModel.coefficients\n",
    "    \n",
    "    # маппинг весов на столбцы\n",
    "    coef_mapping = train.select('features').schema[0].metadata['ml_attr']['attrs']['numeric']\n",
    "    \n",
    "    # сопоставляем веса и имена столбцов\n",
    "    weights_from_model = {m['name']: round(float(coeffs[m['idx']]),0) for m in coef_mapping}\n",
    "    #weights_from_model = {col : round(float(w),0) for col, w in zip(dataset.schema.names[2:], lrModel.coefficients)}\n",
    "    \n",
    "    for risk in typology['risks']:\n",
    "        # пропускаем fct_shady_risk. Оставляем инициализацию по умолчанию\n",
    "        if risk['table_name'] == 'fct_shady_risks':\n",
    "            continue\n",
    "            \n",
    "        risk['weight'] = weights_from_model[f\"{risk['table_name']}__{risk['column_name']}\"]\n",
    "    \n",
    "    # пороговое значение\n",
    "    medium_ppm = int(model['threshold'] / 2 - uncertanity)\n",
    "    high_ppm = int(model['threshold'] / 2 + uncertanity)\n",
    "\n",
    "    # Построение ROC - кривой\n",
    "    predictions = lrModel.transform(test)\n",
    "    predictions = predictions.select('client_id', 'label', 'prediction')\n",
    "    \n",
    "    y_true = predictions.select(\"label\").toPandas() > 0\n",
    "    \n",
    "    probs = predictions.select('prediction').toPandas() / 1000\n",
    "    \n",
    "    print(model[\"id\"], model[\"description\"])\n",
    "    \n",
    "    plot_roc_curve(y_true, probs, typology['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f39ec0e",
   "metadata": {},
   "source": [
    "## Load weights to JSON-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9147fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = str(datetime.now().replace(microsecond=0)).replace(\" \", \"_\")\n",
    "\n",
    "filename = f\"../FR/PythonProjects/ML_weights/weights_{now}.json\"\n",
    "\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump({'data': weights}, f)\n",
    "    \n",
    "print(f\"Файл: {filename} - сохранен!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
