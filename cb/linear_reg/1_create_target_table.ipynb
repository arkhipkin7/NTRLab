{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0d37aa9",
   "metadata": {},
   "source": [
    "## UDF Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d99ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"В этом параграфе определяются функции KYC. Выполните его перед своим SQL запросом\")\n",
    "\n",
    "import org.apache.spark.sql.api.java.UDF1\n",
    "import java.io.Serializable\n",
    "\n",
    "object BaseXX {\n",
    "  import java.math.BigInteger\n",
    " \n",
    "  val MAX128 = BigInteger.valueOf(1).shiftLeft(128)\n",
    "  //for base58:\n",
    "  val CODE_STRING = \"123456789ABCDEFGHJKLMNPQRSTUVWXYZabcdefghijkmnopqrstuvwxyz\"\n",
    "  val NXX = BigInteger.valueOf(CODE_STRING.length)\n",
    "\n",
    "  def encode(source: Array[Byte]): String = {\n",
    "    val x = new BigInteger(source)\n",
    "    encode(x)\n",
    "  }\n",
    "\n",
    "  def encode(x: BigInteger): String = {\n",
    "    var r: BigInteger = x \n",
    "    val sb = new StringBuilder\n",
    "    if (r.signum < 0) r = MAX128.add(r.abs).abs\n",
    "    while (r.signum > 0) {\n",
    "      sb.append(CODE_STRING.charAt(r.mod(NXX).abs.intValue))\n",
    "      r = r.divide(NXX)\n",
    "    }\n",
    "    sb.reverse.toString\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "class Key1Hash extends UDF1[String, String] with Serializable {\n",
    "  import java.math.BigInteger\n",
    "  import java.nio.charset.StandardCharsets\n",
    "  import java.security.{MessageDigest, NoSuchAlgorithmException}\n",
    "\n",
    "  @throws[Exception]\n",
    "  override def call(t1: String): String = try if (t1 != null && t1.length < 20 && t1.forall(x => x >= '0' && x <= '9')) BaseXX.encode(new BigInteger(t1))\n",
    "  else if (t1 != null && t1.length <= 80) {\n",
    "    val id = BaseXX.encode(t1.getBytes(StandardCharsets.UTF_8))\n",
    "    if (id.length < 40) id\n",
    "    else BaseXX.encode(MessageDigest.getInstance(\"SHA-256\").digest(String.valueOf(t1).getBytes(StandardCharsets.UTF_8)))\n",
    "  }\n",
    "  else BaseXX.encode(MessageDigest.getInstance(\"SHA-256\").digest(String.valueOf(t1).getBytes(StandardCharsets.UTF_8)))\n",
    "  catch {\n",
    "    case e: NoSuchAlgorithmException =>\n",
    "      e.printStackTrace()\n",
    "      throw e\n",
    "  }\n",
    "}\n",
    "\n",
    "object KbkNdfl {\n",
    "   val ndflKbk =\n",
    "    \"\"\"\n",
    "      |# ____  __._____.___._________\n",
    "      |#|    |/ _|\\__  |   |\\_   ___ \\\n",
    "      |#|      <   /   |   |/    \\  \\/\n",
    "      |#|    |  \\  \\____   |\\     \\____\n",
    "      |#|____|__ \\ / ______| \\______  /\n",
    "      |#        \\/ \\/               \\/\n",
    "      |# КБК НДФЛ\n",
    "      |182 1 01 02010 01 1000 110\n",
    "      |182 1 01 02010 01 2100 110\n",
    "      |182 1 01 02010 01 2200 110\n",
    "      |182 1 01 02010 01 3000 110\n",
    "      |182 1 01 02050 01 2100 110\n",
    "      |182 1 01 02050 01 2200 110\n",
    "      |182 1 01 02050 01 3000 110\n",
    "      |\n",
    "      |\"\"\".stripMargin.split(\"\\n\").filter((x: String) => !x.startsWith(\"#\")).map((x: String) => x.replaceAll(\"\\\\s+\", \"\"))\n",
    "      .toSet\n",
    "}\n",
    "\n",
    "object KbkInsurance {\n",
    "  val insuranceKbk =\n",
    "    \"\"\"\n",
    "      |# ____  __._____.___._________\n",
    "      |#|    |/ _|\\__  |   |\\_   ___ \\\n",
    "      |#|      <   /   |   |/    \\  \\/\n",
    "      |#|    |  \\  \\____   |\\     \\____\n",
    "      |#|____|__ \\ / ______| \\______  /\n",
    "      |#        \\/ \\/               \\/\n",
    "      |# КБК страховых взносов\n",
    "      |182 1 02 02010 06 1000 160\n",
    "      |182 1 02 02010 06 2100 160\n",
    "      |182 1 02 02010 06 2200 160\n",
    "      |182 1 02 02010 06 3000 160\n",
    "      |182 1 02 02010 06 1010 160\n",
    "      |182 1 02 02010 06 2110 160\n",
    "      |182 1 02 02010 06 2210 160\n",
    "      |182 1 02 02010 06 3010 160\n",
    "      |182 1 02 02020 06 1000 160\n",
    "      |182 1 02 02020 06 2100 160\n",
    "      |182 1 02 02020 06 2200 160\n",
    "      |182 1 02 02020 06 3000 160\n",
    "      |182 1 02 02031 06 1000 160\n",
    "      |182 1 02 02031 06 2100 160\n",
    "      |182 1 02 02031 06 2200 160\n",
    "      |182 1 02 02031 06 3000 160\n",
    "      |182 1 02 02032 06 1000 160\n",
    "      |182 1 02 02032 06 2100 160\n",
    "      |182 1 02 02032 06 2200 160\n",
    "      |182 1 02 02032 06 3000 160\n",
    "      |182 1 02 02080 06 1000 160\n",
    "      |182 1 02 02080 06 2100 160\n",
    "      |182 1 02 02080 06 2200 160\n",
    "      |182 1 02 02080 06 3000 160\n",
    "      |182 1 02 02090 07 1000 160\n",
    "      |182 1 02 02090 07 2100 160\n",
    "      |182 1 02 02090 07 2200 160\n",
    "      |182 1 02 02090 07 3000 160\n",
    "      |182 1 02 02090 07 1010 160\n",
    "      |182 1 02 02090 07 2110 160\n",
    "      |182 1 02 02090 07 2210 160\n",
    "      |182 1 02 02090 07 3010 160\n",
    "      |182 1 02 02100 06 1000 160\n",
    "      |182 1 02 02100 06 2100 160\n",
    "      |182 1 02 02100 06 2200 160\n",
    "      |182 1 02 02100 06 3000 160\n",
    "      |182 1 02 02101 08 1011 160\n",
    "      |182 1 02 02101 08 2011 160\n",
    "      |182 1 02 02101 08 3011 160\n",
    "      |182 1 02 02101 08 1013 160\n",
    "      |182 1 02 02101 08 2013 160\n",
    "      |182 1 02 02101 08 2213 160\n",
    "      |182 1 02 02101 08 3013 160\n",
    "      |182 1 02 02110 06 2100 160\n",
    "      |182 1 02 02110 06 2200 160\n",
    "      |182 1 02 02110 06 3000 160\n",
    "      |182 1 02 02120 06 1000 160\n",
    "      |182 1 02 02120 06 2100 160\n",
    "      |182 1 02 02120 06 2200 160\n",
    "      |182 1 02 02120 06 3000 160\n",
    "      |182 1 02 02131 06 1010 160\n",
    "      |182 1 02 02131 06 1020 160\n",
    "      |182 1 02 02132 06 1010 160\n",
    "      |182 1 02 02132 06 1020 160\n",
    "      |182 1 02 02131 06 2110 160\n",
    "      |182 1 02 02131 06 3010 160\n",
    "      |182 1 02 02132 06 2110 160\n",
    "      |182 1 02 02132 06 2210 160\n",
    "      |182 1 02 02132 06 3010 160\n",
    "      |182 1 02 02131 06 2100 160\n",
    "      |182 1 02 02131 06 2200 160\n",
    "      |182 1 02 02131 06 3000 160\n",
    "      |182 1 02 02132 06 2100 160\n",
    "      |182 1 02 02131 06 2210 160\n",
    "      |182 1 02 02132 06 3000 160\n",
    "      |182 1 02 02101 08 2211 160\n",
    "      |182 1 02 02131 06 2100 160\n",
    "      |182 1 02 02131 06 2110 160\n",
    "      |182 1 02 02131 06 2200 160\n",
    "      |182 1 02 02131 06 2210 160\n",
    "      |182 1 02 02131 06 3000 160\n",
    "      |182 1 02 02131 06 3010 160\n",
    "      |182 1 02 02132 06 2100 160\n",
    "      |182 1 02 02132 06 2110 160\n",
    "      |182 1 02 02132 06 2200 160\n",
    "      |182 1 02 02132 06 2210 160\n",
    "      |182 1 02 02132 06 3000 160\n",
    "      |182 1 02 02132 06 3010 160\n",
    "      |\"\"\".stripMargin.split(\"\\n\").filter((x: String) => !x.startsWith(\"#\")).map((x: String) => x.replaceAll(\"\\\\s+\", \"\"))\n",
    "      .toSet\n",
    "}\n",
    "\n",
    "import org.apache.spark.sql.types.DataTypes\n",
    "\n",
    "spark.udf.register(\"myid\", new Key1Hash, DataTypes.StringType)\n",
    "spark.udf.register(\"kbkNdfl\", (x:String)=>Option.apply(x).map(v=>KbkNdfl.ndflKbk.contains(v.replaceAll(\"\\\\s+\", \"\"))))\n",
    "spark.udf.register(\"kbkInsurance\", (x:String)=>Option.apply(x).map(v=>KbkInsurance.insuranceKbk.contains(v.replaceAll(\"\\\\s+\", \"\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756875f9",
   "metadata": {},
   "source": [
    "## Объявление глобальных переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5b917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "\n",
    "period = 90                       # дней, за которые смотрится активность красного клиента\n",
    "max_score = 1000                  # Максимальное значение риска (Красный Клиент)\n",
    "general_typology = 9              # Общая типология \"Красный клиент\".\n",
    "sample_size = None                # количество красных (и зеленых) клиентов, которые берутся в работу None - для максимального количества\n",
    "KycDM = \"zt_dm_kyc_data\"          # Схема KYC, где лежит dim_client. Только на чтение\n",
    "arfsDM = \"zt_dm_aso_dfm_arfs\"     # Схема ARFS, с правами на запись. Таблицы создаются здесь\n",
    "target_table = \"tmp_target_table\" # название целевой таблицы\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f26f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "spark.sql(f\"select * from {KycDM}.clc_client_risks\").limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87d2c35",
   "metadata": {},
   "source": [
    "## Получение красных клиентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3a6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pyspark\n",
    "\n",
    "sql_approved_risks = \"\"\"\n",
    "    with actual_approved_risks as (\n",
    "        select id, client_id, left(main_doubful_code, 1) as tipologia, deleted, row_number() over(partition by id order by version desc) as rn\n",
    "        from {KycDM}.fct_approved_risks\n",
    "        where left(main_doubtful_code, 2) != '99' and left(main_doubtful_code, 2) != '10')\n",
    "    ), actual_dim_list as (\n",
    "        select distinct right(left(id, 3), 1) as tipilogia, \n",
    "            case when instr(descreption, '.') > 0 then replace(description, substring(description, instr(description, '.'), lenght(description) - instr(description, '.') + 1), '') else description end as description,\n",
    "            deleted, row_number() over(partition by id order by version desc) as rn\n",
    "        from {KycDM}.dim_list\n",
    "        where right(left(id, 3), 2) != '99' and right(left(id, 3), 2) != '10'\n",
    "    ), actual_dim_client as (\n",
    "        select id, inn, client_type\n",
    "        from {KycDM}.dim_client\n",
    "        where is_actual is True and delete is False\n",
    "    ), actual_client as (\n",
    "        select \n",
    "            distinct operation_client_id as oci,\n",
    "            date_add(max(operation_dt) over(partition by operation_client_id), 1) as last_operation_dt\n",
    "        from {KycDM}.client_payment_l\n",
    "    ), tmp as (\n",
    "        select\n",
    "            adc.inn,\n",
    "            arc.tipologia,\n",
    "            adc.client_type,\n",
    "            adl.description\n",
    "            ac.last_operation_dt\n",
    "        from actual_approved_risks as aar\n",
    "        inner join actual_client ac in ac.oci = aar.client_id\n",
    "        inner join actual_dim_client adc on adc.id = aar.client_id\n",
    "        inner join actuail_dim_list adl on aar.tipologia = adl.tipologia\n",
    "        where not aar.deleted and aar.rn = 1 and not adl.deleted and adl.rn = 1\n",
    "    )\n",
    "    select tmp.*, dc.id as calendar_id\n",
    "    from tmp \n",
    "    left join {KycDM}.dim_calendar dc on tmp.last_oparation_dt = dc.day\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "reds = spark.sql(sql_approved_risks.format(schema = KycDM, period = period))\n",
    "\n",
    "reds.write.mode(\"overwrite\").saveAsTable(f\"{arfsDM}.tmp_red_clients\")\n",
    "reds = spark.sql(f\"select * from {arfsDM}.tmp_red_clients\")\n",
    "\n",
    "reds.orderBy(rand()).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba772b81",
   "metadata": {},
   "source": [
    "## Добавление ОБЩЕЙ типологии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f330204",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "# Проверим что ее нет в таблице красных\n",
    "if reds.filter(f\"tipologia={general_typology}\").count()>0:\n",
    "    print(f\"ВНИМАНИЕ! в таблице УЖЕ содержатся клиенты с типологией {general_typology}!!!!!!!!\")\n",
    "    raise \"\"\n",
    "common_red = spark.sql(f\"select distinct inn, {general_typology} as tipologia, 'Обобщенная'  as description from {arfsDM}.tmp_red_clients\")\n",
    "common_red.write.mode(\"append\").saveAsTable(f\"{arfsDM}.tmp_red_clients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee50a93",
   "metadata": {},
   "source": [
    "## Обзор представленности по типологиям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc28983",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "print(f\"Всего красных: {reds.count()}\")\n",
    "reds.groupBy('tipologia','description').count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4ea6e0",
   "metadata": {},
   "source": [
    "## Составление красных и Зеленых выборок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "# красные клиенты \n",
    "# !!!! проверить на соответствие сырой таблице!!!!!!!\n",
    "# ДОРАБОТАТЬ исключение из \"зеленой зоны\"\n",
    "red_df = spark.sql(f\"SELECT myid(inn) as client_id, {max_score} AS label, tipologia as dubious_id FROM {arfsDM}.tmp_red_clients\")\n",
    "\n",
    "# все клиенты входящие в hw_inn_list\n",
    "not_greens = spark.sql(f\"SELECT distinct inn, -1 as label, -1 as dubious_id from {KycDM}.hw_inn_lists\")\n",
    "print(f\"Not greens from inn lists: {not_greens.count()}\")\n",
    "\n",
    "affilated  = spark.sql(f\"SELECT distinct dc.inn, -1 as label, -1 as dubious_id from {KycDM}.fct_shady_aff_risks aff inner join {KycDM}.dim_client dc on aff.client_id = dc.id\")\n",
    "print(f\"Affilated: {affilated.count()}\")\n",
    "not_greens = not_greens.union(affilated).distinct()\n",
    "\n",
    "print(f\"RED: {red_df.count()} \\nTotal NOT green: {not_greens.count()}\")\n",
    "\n",
    "# Зеленые клиенты - все те, кто есть в dim_client но не являются красными или желтыми. только ЮЛ\n",
    "green_df = spark.sql(f\"select distinct id as client_id, inn from {KycDM}.dim_client where client_type='ЮЛ'\")\n",
    "print(f\"Total in dim_client: {green_df.count()}\")\n",
    "\n",
    "green_df = green_df.join(not_greens, on=['inn'], how='left')\n",
    "# print(f\"after join {green_df.count()}\")\n",
    "\n",
    "green_df = green_df.fillna(0).filter('label=0')\n",
    "# print(f\"after filter {green_df.count()}\")\n",
    "\n",
    "green_df = green_df.select('client_id', 'label', 'dubious_id')\n",
    "print(f\"GREEN: {green_df.count()}\")\n",
    "print(\"\\n\", \"Greens example:\")\n",
    "green_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff8c1ea",
   "metadata": {},
   "source": [
    "## Обрезка, балансировка и сохранение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06585efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "# вычисление долей\n",
    "red_count = red_df.count()\n",
    "\n",
    "if sample_size and red_count>sample_size:\n",
    "    red_fraction = sample_size/red_count\n",
    "else:\n",
    "    red_fraction = 1\n",
    "\n",
    "green_fraction = red_count/green_df.count() * 1.1 # Зеленых с небольшим запасом\n",
    "    \n",
    "# случайная подвыборка красных и зеленых\n",
    "if red_fraction==1:\n",
    "    red_subset = red_df\n",
    "else:\n",
    "    red_subset = red_df.sample(red_fraction)\n",
    "    \n",
    "green_subset = green_df.sample(green_fraction)\n",
    "\n",
    "target_df = red_subset.union(green_subset)\n",
    "\n",
    "# сохранение объединенной сбалансированной таблицы\n",
    "target_df.write.mode(\"overwrite\").saveAsTable(f\"{arfsDM}.{target_table}\")\n",
    "target_row_count = target_df.count()\n",
    "\n",
    "print(f\"Saved {target_row_count} rows to {arfsDM}.{target_table}\")\n",
    "spark.sql(f\"select * from {arfsDM}.{target_table}\").groupBy('dubious_id', 'label').count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3948e0e",
   "metadata": {},
   "source": [
    "## Обзор полученной таблицы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b6119",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "dataset = spark.sql(f\"select * from  {arfsDM}.{target_table}\")\n",
    "print(f\"Размер датасета: {dataset.count()}\\n\")\n",
    "dataset.orderBy(rand()).limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046d9c3e",
   "metadata": {},
   "source": [
    "## Сохраняем только зеленых в таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31ba4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "print(green_df.count())\n",
    "green_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdce623",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "green_df.write.mode(\"overwrite\").saveAsTable(f\"{arfsDM}.tmp_green_client_table\")\n",
    "print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
